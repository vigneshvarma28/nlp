{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4e257d",
   "metadata": {},
   "source": [
    "# HACKERRANK SOLUTIONS-2211CS020360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd5f15",
   "metadata": {},
   "source": [
    "# 1)Correct the Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc82b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "hwllo word\n",
      "Iam goin to chna\n",
      "hello world\n",
      "Iam going to china\n"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "import json\n",
    "from difflib import get_close_matches\n",
    "\n",
    "word_list=[\"going\",\"to\",\"china\",\"hello\",\"world\",\"from\",\"algorithm\",\"python\",\"programming\",\"challenge\",\"query\",\"misspelled\",\"correct\"]\n",
    "\n",
    "compressed_dict=zlib.compress(json.dumps(word_list).encode())\n",
    "\n",
    "def load_dict():\n",
    "    return set(json.loads(zlib.decompress(compressed_dict).decode()))\n",
    "    \n",
    "def correct_word(word,dictionary):\n",
    "    if word in dictionary:\n",
    "        return word\n",
    "    matches=get_close_matches(word,dictionary,n=1,cutoff=0.8)\n",
    "    return matches[0] if matches else word\n",
    "    \n",
    "def correcy_query(query,dictionary):\n",
    "    words=query.split()\n",
    "    corrected_words=[correct_word(word,dictionary) for word in words]\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def process_queries(queries):\n",
    "    dictionary=load_dict()\n",
    "    return [correcy_query(query,dictionary) for query in queries]\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    N=int(input())\n",
    "    queries=[input() for _ in range(N)]\n",
    "    \n",
    "    rectified_queries=process_queries(queries)\n",
    "    for query in rectified_queries:\n",
    "        print(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377ce82",
   "metadata": {},
   "source": [
    "# 2)Deterministic Url and HashTag Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6d4b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "#isittime  \n",
      "Segmentation for Input: isittime\n",
      "www.whatismyname.com\n",
      "Segmentation for Input: whatismyname\n",
      "#letusgo\n",
      "Segmentation for Input: letusgo\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize(input_string, dictionary):\n",
    "    length = len(input_string)\n",
    "    if length == 0:\n",
    "        return []\n",
    "\n",
    "    dp = [None] * (length + 1)\n",
    "    dp[0] = []\n",
    "\n",
    "    for i in range(1, length + 1):\n",
    "        for j in range(i):\n",
    "            left_part = input_string[j:i]\n",
    "            if (left_part in dictionary or is_number(left_part)) and dp[j] is not None:\n",
    "                right_part_tokens = dp[j] + [left_part]\n",
    "                if dp[i] is None or len(right_part_tokens) > len(dp[i]):\n",
    "                    dp[i] = right_part_tokens\n",
    "\n",
    "    return dp[length] if dp[length] is not None else [input_string]\n",
    "\n",
    "def main():\n",
    "    num_test_cases = int(input())\n",
    "    for _ in range(num_test_cases):\n",
    "        input_string = input().strip().lower()\n",
    "        if input_string.startswith(\"www.\"):\n",
    "            input_string = input_string[4:].rsplit(\".\", 1)[0]\n",
    "        elif input_string.startswith(\"#\"):\n",
    "            input_string = input_string[1:]\n",
    "\n",
    "        tokens = tokenize(input_string, dictionary)\n",
    "        print(f\"Segmentation for Input: {' '.join(tokens)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"words.txt\", \"r\") as file:\n",
    "        dictionary = set(word.strip().lower() for word in file.readlines())\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb35a2",
   "metadata": {},
   "source": [
    "# 3)Disambiguation: Mouse vs Mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f2c4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The house was infested with mice.\n",
      "animal\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "training_sentences = [\n",
    "    \"The complete mouse reference genome was sequenced in 2002.\",\n",
    "    \"Tail length varies according to the environmental temperature of the mouse during postnatal development.\",\n",
    "    \"A mouse is an input device.\",\n",
    "    \"Many mice have a pink tail.\",\n",
    "    \"The mouse pointer on the screen helps in navigation.\",\n",
    "    \"A rodent like a mouse has sharp teeth.\",\n",
    "    \"The mouse was connected to the computer using a USB port.\",\n",
    "    \"The house was infested with mice.\",\n",
    "    \"Computer users often prefer a wireless mouse.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"animal\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\",\n",
    "    \"animal\",\n",
    "    \"computer-mouse\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(training_sentences)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, labels)\n",
    "\n",
    "def predict_mouse_type(sentence):\n",
    "    vectorized_sentence = vectorizer.transform([sentence])\n",
    "    prediction = classifier.predict(vectorized_sentence)[0]\n",
    "    return prediction\n",
    "\n",
    "num_test_cases = int(input())\n",
    "for _ in range(num_test_cases):\n",
    "    sentence = input()\n",
    "    prediction = predict_mouse_type(sentence)\n",
    "    print(prediction)\n",
    "\n",
    "with open('mouse_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump((vectorizer, classifier), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03aeb7d",
   "metadata": {},
   "source": [
    "# 4)Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509e033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: French\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def normalize_to_ascii(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "training_texts = {\n",
    "    \"English\": [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Rip Van Winkle is a story set in the years before the American Revolutionary War.\",\n",
    "    ],\n",
    "    \"French\": [\n",
    "        \"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
    "        \"La revolution francaise a marque une periode importante de l'histoire.\",\n",
    "    ],\n",
    "    \"German\": [\n",
    "        \"Der schnelle braune Fuchs springt uber den faulen Hund.\",\n",
    "        \"Die deutsche Wiedervereinigung war ein historisches Ereignis.\",\n",
    "    ],\n",
    "    \"Spanish\": [\n",
    "        \"El rapido zorro marron salta sobre el perro perezoso.\",\n",
    "        \"La Revolucion Espanola fue un momento clave en la historia. Si quieres que te asciendan te tienes que poner las pilas.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "for language, samples in training_texts.items():\n",
    "    labels.extend([language] * len(samples))\n",
    "    texts.extend([normalize_to_ascii(sample) for sample in samples])\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 4), analyzer=\"char\")\n",
    "X_train = vectorizer.fit_transform(texts)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, labels)\n",
    "\n",
    "with open(\"language_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump((vectorizer, classifier), model_file)\n",
    "\n",
    "def detect_language(snippet):\n",
    "    with open(\"language_model.pkl\", \"rb\") as model_file:\n",
    "        vectorizer, classifier = pickle.load(model_file)\n",
    "    snippet = normalize_to_ascii(snippet)\n",
    "    X_test = vectorizer.transform([snippet])\n",
    "    prediction = classifier.predict(X_test)\n",
    "    return prediction[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    snippet = \"\"\"Le renard brun rapide saute par-dessus le chien paresseux.\"\"\"\n",
    "    detected_language = detect_language(snippet.strip())\n",
    "    print(f\"Detected Language: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cfd51",
   "metadata": {},
   "source": [
    "# 5)The Missing Apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3766fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At a new's conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on hi's previou's trip into space in 2010 \"I even asked our psychological support folk's to send me a calendar with photograph's of nature, of rivers, of woods, of lakes.\" Kelly wa's asked if hed mis's hi's twin brother Mark, who also wa's an astronaut. \"Were used to thi's kind of thing,\" he said. \"Ive gone longer without seeing him and it wa's great.\" The mission won't be the longest time that a human ha's spent in space - four Russian's spent a year or more aboard the Soviet-built Mir space station in the 1990s. SCI Astronaut Twin's Scott Kelly (left) wa's asked Thursday if hed mis's hi's twin brother, Mark, who also wa's an astronaut. we're used to thi's kind of thing, he said. I've gone longer without seeing him and it wa's great. (NASA/Associated Press) \"The last time we had such a long duration flight wa's almost 20 year's and of course all ... scientific technique's are more advanced than 20 year's ago and right now we need to test the capability of a human being to perform such long-duration flights. So thi's i's the main objective of our flight, to test ourselves,\" said Kornienko.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def restore_apostrophes(text):\n",
    "    restored_text = []\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        lower_word = word.lower()\n",
    "        if lower_word == \"dont\":\n",
    "            restored_text.append(\"don't\")\n",
    "        elif lower_word == \"wont\":\n",
    "            restored_text.append(\"won't\")\n",
    "        elif lower_word == \"cant\":\n",
    "            restored_text.append(\"can't\")\n",
    "        elif lower_word == \"isnt\":\n",
    "            restored_text.append(\"isn't\")\n",
    "        elif lower_word == \"arent\":\n",
    "            restored_text.append(\"aren't\")\n",
    "        elif lower_word == \"wasnt\":\n",
    "            restored_text.append(\"wasn't\")\n",
    "        elif lower_word == \"werent\":\n",
    "            restored_text.append(\"weren't\")\n",
    "        elif lower_word == \"hasnt\":\n",
    "            restored_text.append(\"hasn't\")\n",
    "        elif lower_word == \"havent\":\n",
    "            restored_text.append(\"haven't\")\n",
    "        elif lower_word == \"hadnt\":\n",
    "            restored_text.append(\"hadn't\")\n",
    "        elif lower_word == \"didnt\":\n",
    "            restored_text.append(\"didn't\")\n",
    "        elif lower_word == \"ive\":\n",
    "            restored_text.append(\"I've\")\n",
    "        elif lower_word == \"were\":\n",
    "            restored_text.append(\"we're\")\n",
    "        elif lower_word == \"i\":\n",
    "            restored_text.append(\"I\")\n",
    "        elif lower_word == \"id\":\n",
    "            restored_text.append(\"I'd\")\n",
    "        elif lower_word == \"youve\":\n",
    "            restored_text.append(\"you've\")\n",
    "        elif lower_word == \"hes\":\n",
    "            restored_text.append(\"he's\")\n",
    "        elif lower_word == \"shes\":\n",
    "            restored_text.append(\"she's\")\n",
    "        elif lower_word == \"its\":\n",
    "            restored_text.append(\"it's\")\n",
    "        elif re.match(r'\\w+s$', word) and lower_word not in [\"its\", \"hers\", \"ours\", \"yours\", \"theirs\"]:\n",
    "            restored_text.append(re.sub(r\"s$\", \"'s\", word))\n",
    "        else:\n",
    "            restored_text.append(word)\n",
    "\n",
    "    return \" \".join(restored_text)\n",
    "\n",
    "input_text = \"\"\"At a news conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on his previous trip into space in 2010 \"I even asked our psychological support folks to send me a calendar with photographs of nature, of rivers, of woods, of lakes.\" Kelly was asked if hed miss his twin brother Mark, who also was an astronaut. \"Were used to this kind of thing,\" he said. \"Ive gone longer without seeing him and it was great.\" The mission wont be the longest time that a human has spent in space - four Russians spent a year or more aboard the Soviet-built Mir space station in the 1990s. SCI Astronaut Twins Scott Kelly (left) was asked Thursday if hed miss his twin brother, Mark, who also was an astronaut. Were used to this kind of thing, he said. Ive gone longer without seeing him and it was great. (NASA/Associated Press) \"The last time we had such a long duration flight was almost 20 years and of course all ... scientific techniques are more advanced than 20 years ago and right now we need to test the capability of a human being to perform such long-duration flights. So this is the main objective of our flight, to test ourselves,\" said Kornienko.\"\"\"\n",
    "\n",
    "output_text = restore_apostrophes(input_text)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22f059",
   "metadata": {},
   "source": [
    "# 6)Segment the Twitter Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9146aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "wearethepeople   \n",
      "mentionyourfaves\n",
      "nowplaying   \n",
      "thewalkingdead   \n",
      "followme\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n"
     ]
    }
   ],
   "source": [
    "def segment_hashtag(hashtag, word_dict):\n",
    "    n = len(hashtag)\n",
    "    dp = [None] * (n + 1)\n",
    "    dp[0] = []\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(max(0, i - 20), i):\n",
    "            word = hashtag[j:i]\n",
    "            if word in word_dict and dp[j] is not None:\n",
    "                dp[i] = dp[j] + [word]\n",
    "                break\n",
    "\n",
    "    return \" \".join(dp[n]) if dp[n] is not None else hashtag\n",
    "\n",
    "def process_hashtags(num_hashtags, hashtags, word_dict):\n",
    "    result = []\n",
    "    for hashtag in hashtags:\n",
    "        segmented = segment_hashtag(hashtag, word_dict)\n",
    "        result.append(segmented)\n",
    "    return result\n",
    "\n",
    "word_dict = {\n",
    "    \"we\", \"are\", \"the\", \"people\", \"mention\", \"your\", \"faves\",\n",
    "    \"now\", \"playing\", \"walking\", \"dead\", \"follow\", \"me\"\n",
    "}\n",
    "\n",
    "num_hashtags = int(input())\n",
    "hashtags = [input().strip() for _ in range(num_hashtags)]\n",
    "\n",
    "segmented_hashtags = process_hashtags(num_hashtags, hashtags, word_dict)\n",
    "for segmented in segmented_hashtags:\n",
    "    print(segmented)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbdb12",
   "metadata": {},
   "source": [
    "# 7)Expand the Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e8f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "The United Nations Children's Fund (UNICEF) is a United Nations Programme headquartered in New York City, that provides long-term humanitarian and developmental assistance to children and mothers in developing countries.\n",
      "The National University of Singapore is a leading global university located in Singapore, Southeast Asia. NUS is Singapore's flagship university which offers a global approach to education and research.\n",
      "Massachusetts Institute of Technology (MIT) is a private research university located in Cambridge, Massachusetts, United States.\n",
      "NUS\n",
      "MIT\n",
      "UNICEF\n",
      "located in Singapore, Southeast Asia.\n",
      "Massachusetts Institute of Technology\n",
      "The United Nations Children's Fund\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_acronyms_and_expansions(snippets):\n",
    "    acronym_dict = {}\n",
    "    for snippet in snippets:\n",
    "        matches = re.findall(r'\\((\\b[A-Z]+\\b)\\)', snippet)\n",
    "        for match in matches:\n",
    "            preceding_text = snippet.split(f\"({match})\")[0].strip()\n",
    "            expansion_candidates = re.split(r'[.,;:-]', preceding_text)\n",
    "            if expansion_candidates:\n",
    "                expansion = expansion_candidates[-1].strip()\n",
    "                acronym_dict[match] = expansion\n",
    "\n",
    "        words = snippet.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.isupper() and len(word) > 1:\n",
    "                if word not in acronym_dict:\n",
    "                    if i > 0:\n",
    "                        preceding_context = \" \".join(words[max(0, i-5):i])\n",
    "                        acronym_dict[word] = preceding_context\n",
    "    return acronym_dict\n",
    "\n",
    "def process_tests(acronym_dict, tests):\n",
    "    results = []\n",
    "    for test in tests:\n",
    "        expansion = acronym_dict.get(test.upper(), \"Not Found\")\n",
    "        results.append(expansion)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    n = int(input().strip())\n",
    "    snippets = [input().strip() for _ in range(n)]\n",
    "    tests = [input().strip() for _ in range(n)]\n",
    "    acronym_dict = extract_acronyms_and_expansions(snippets)\n",
    "    results = process_tests(acronym_dict, tests)\n",
    "    print(\"\\n\".join(results))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422961a",
   "metadata": {},
   "source": [
    "# 8)Correct the Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bddfd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hell iam gong too hyderabad\n",
      "hello iam going to hyderabad\n"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "import json\n",
    "from difflib import get_close_matches\n",
    "\n",
    "word_list=[\"going\",\"to\",\"china\",\"hello\",\"world\",\"from\",\"algorithm\",\"python\",\"programming\",\"challenge\",\"query\",\"misspelled\",\"correct\"]\n",
    "\n",
    "compressed_dict=zlib.compress(json.dumps(word_list).encode())\n",
    "\n",
    "def load_dict():\n",
    "    return set(json.loads(zlib.decompress(compressed_dict).decode()))\n",
    "    \n",
    "def correct_word(word,dictionary):\n",
    "    if word in dictionary:\n",
    "        return word\n",
    "    matches=get_close_matches(word,dictionary,n=1,cutoff=0.8)\n",
    "    return matches[0] if matches else word\n",
    "    \n",
    "def correcy_query(query,dictionary):\n",
    "    words=query.split()\n",
    "    corrected_words=[correct_word(word,dictionary) for word in words]\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def process_queries(queries):\n",
    "    dictionary=load_dict()\n",
    "    return [correcy_query(query,dictionary) for query in queries]\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    N=int(input())\n",
    "    queries=[input() for _ in range(N)]\n",
    "    \n",
    "    rectified_queries=process_queries(queries)\n",
    "    for query in rectified_queries:\n",
    "        print(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539f173",
   "metadata": {},
   "source": [
    "# 9)A Text-Processing Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04932cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "I visited the Eiffel Tower on 15th of August 2023.\n",
      "She plans to meet him on 12/25/2024 for Christmas.\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def count_articles_and_dates(fragment):\n",
    "    lower_fragment = fragment.lower()\n",
    "    a_count = len(re.findall(r'\\b[a]\\b', lower_fragment))\n",
    "    an_count = len(re.findall(r'\\b[an]\\b', lower_fragment))\n",
    "    the_count = len(re.findall(r'\\b[the]\\b', lower_fragment))\n",
    "\n",
    "    date_patterns = [\n",
    "        r'\\b\\d{1,2}(?:st|nd|rd|th)?(?:\\s+of)?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4}\\b',\n",
    "        r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd)?,?\\s+\\d{2,4}\\b',\n",
    "        r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',\n",
    "        r'\\b\\d{4}-\\d{2}-\\d{2}\\b'\n",
    "    ]\n",
    "    date_regex = '|'.join(date_patterns)\n",
    "    dates = re.findall(date_regex, fragment, re.IGNORECASE)\n",
    "    date_count = len(dates)\n",
    "\n",
    "    return a_count, an_count, the_count, date_count\n",
    "\n",
    "def main():\n",
    "    t = int(input().strip())\n",
    "    fragments = [input().strip() for _ in range(t)]\n",
    "\n",
    "    results = []\n",
    "    for fragment in fragments:\n",
    "        a_count, an_count, the_count, date_count = count_articles_and_dates(fragment)\n",
    "        results.append(f\"{a_count}\\n{an_count}\\n{the_count}\\n{date_count}\")\n",
    "\n",
    "    print(\"\\n\".join(results))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2c9ab",
   "metadata": {},
   "source": [
    "# 10)Who is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d7b99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of lines in the text snippet:  3\n",
      " vignesh roll number is 360\n",
      " gangadhar is a gud boy\n",
      " They both are friends\n",
      "Enter the list of entities (separate by semicolons):  vignesh;gangadhar;They\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def resolve_pronouns(text, entities):\n",
    "    pronoun_pattern = r'\\\\(\\w+)\\\\'\n",
    "    pronouns = [(match.group(1), match.start()) for match in re.finditer(pronoun_pattern, text)]\n",
    "    clean_text = re.sub(r'\\\\(\\w+)\\\\', r'\\1', text)\n",
    "\n",
    "    resolved = []\n",
    "\n",
    "    for pronoun, pos in pronouns:\n",
    "        closest_entity = None\n",
    "        closest_distance = float('inf')\n",
    "\n",
    "        for entity in entities:\n",
    "            entity_pos = clean_text.rfind(entity, 0, pos)\n",
    "            if entity_pos != -1:\n",
    "                distance = pos - (entity_pos + len(entity))\n",
    "                if distance < closest_distance:\n",
    "                    closest_distance = distance\n",
    "                    closest_entity = entity\n",
    "\n",
    "        resolved.append(closest_entity)\n",
    "\n",
    "    return resolved\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        n = int(input(\"Enter the number of lines in the text snippet: \").strip())\n",
    "    except ValueError:\n",
    "        print(\"Error: The first line must contain a valid integer.\")\n",
    "        return\n",
    "    \n",
    "    text_snippet = \"\"\n",
    "    for _ in range(n):\n",
    "        text_snippet += input().strip() + \" \"\n",
    "    \n",
    "    entities_input = input(\"Enter the list of entities (separate by semicolons): \").strip()\n",
    "    entities = [e.strip() for e in entities_input.split(';')]\n",
    "\n",
    "    result = resolve_pronouns(text_snippet, entities)\n",
    "\n",
    "    for entity in result:\n",
    "        print(entity)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cc2a5-7882-48ed-a8ee-2b7a38d95e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
